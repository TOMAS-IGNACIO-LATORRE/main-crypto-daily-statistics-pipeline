# main-crypto-daily-statistics-pipeline

## ğŸ“š Tabla de contenidos

1. [IntroducciÃ³n](#IntroducciÃ³n)
2. [Pipeline de datos](#Pipeline-de-datos)
3. [ImplementaciÃ³n](#ImplementaciÃ³n)

## ğŸŒ IntroducciÃ³n

Este proyecto se encarga de la implementaciÃ³n de un pipeline ETL (Extract, Transform, Load) diseÃ±ado para obtener las cotizaciones de las principales criptomonedas por medio de extracciones diarias del dÃ­a anterior. El pipeline utiliza ![Docker](https://img.shields.io/badge/Docker-blue?logo=docker&logoColor=white) para la contenerizaciÃ³n, ![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-green?logo=apache-airflow&logoColor=white) para la orquestaciÃ³n, ![Amazon Redshift](https://img.shields.io/badge/Amazon%20Redshift-red?logo=amazon-redshift&logoColor=white) como almacenamiento de datos y cÃ³mputo. A su vez, se utiliza ![Python](https://img.shields.io/badge/Python-blue?logo=python&logoColor=white) como lenguaje principal a la hora de gestionar el pipeline extrayendo datos de APIs financieras(CoinGecko y CoinMarketCap), transformando en una etapa posterior y cargando en bases de datos para su uso analÃ­tico.

El objetivo principal de este proyecto es proporcionar un anÃ¡lisis diario de las cotizaciones de las criptomonedas mÃ¡s relevantes, obteniendo los datos del dÃ­a anterior finalizado. A medida que las criptomonedas ganan popularidad y su uso se expande, es esencial monitorear sus precios y tendencias. Este anÃ¡lisis ayuda a los inversores y a otros agentes econÃ³micos a tomar decisiones informadas en un entorno altamente volÃ¡til.

## ğŸ“ˆ Pipeline de datos

En este proceso de ETL, se utilizÃ³ la visiÃ³n de Databricks conocido como [Lakehouse](https://www.databricks.com/glossary/data-lakehouse). En esta visiÃ³n, se inicia con las diversas **fuentes de datos** las cuales se extraen hacia una primera etapa conocida como **Bronze o Staging** en la cual se guardan los datos tal cual provienen de esta fuente de datos primaria. Luego, esa fuente primaria se le lleva hacia una segunda etapa conocida como **Silver** en la cual se aplican ciertas transformaciones como agregado de columnas, modificaciÃ³n de tipos de datos, entre otras mÃ¡s. Para finalizar, estos datos transformados se llevan hacia una etapa final conocida como **Gold** en la cual se disponibilizan los datos para emplear analÃ­tica sea mediante tableros de analÃ­tcas o modelos de Machine Learning. A continuaciÃ³n, comparto una imagen que muestra esta estructura:

![](https://blog.bismart.com/hs-fs/hubfs/Arquitectura_Medallion_Pasos.jpg?width=1754&height=656&name=Arquitectura_Medallion_Pasos.jpg)

### ğŸ“ Fuente de datos hacia Staging
Para todas las fuentes, se utilizÃ³ cÃ³digo en Python para obtener datos de las APIs. Estas son las siguientes:
  -  [CoinMarkerCap API](https://coinmarketcap.com/api/documentation/v1/): Desde esta fuente de datos obtenemos 

> Es importante destacar que esta APIs proporcionan la informaciÃ³n del dÃ­a anterior para cada tipo de cambio, no datos histÃ³ricos de precios. Si se desea obtener datos de dÃ­as anteriores, se debe modificar el parÃ¡metro days ubicado en la carpeta staging en el archivo `api_extract_data.py`.

### ğŸ“ Staging hacia Silver

### ğŸ“ Silver hacia Gold

### Alertas - email

## ğŸ› ï¸ ImplementaciÃ³n

###  Requisitos previos

Se debe tener instalado las siguientes herramientas:

- Python
- Docker Desktop
- Airflow
- AWS Redshift

## Setup





