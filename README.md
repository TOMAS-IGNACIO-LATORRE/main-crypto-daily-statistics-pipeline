# main-crypto-daily-statistics-pipeline

## üìö Tabla de contenidos

1. [Introducci√≥n](#Introducci√≥n)
2. [Implementaci√≥n](#Implementaci√≥n) 
3. [Pipeline de datos](#Pipeline-de-datos)

## üõ†Ô∏è Implementaci√≥n

###  Requisitos previos

Se debe tener instalado las siguientes herramientas:

- Python
- Docker Desktop
- Airflow
- AWS Redshift

## Setup
Los pasos a seguir son los siguientes:
 #### 1. Clonar este repositorio
   ```bash
    git clone https://github.com/TOMAS-IGNACIO-LATORRE/main-crypto-daily-statistics-pipeline.git
  ```
 #### 2. Navegar en este directorio:
```bash
cd main-crypto-daily-statistics-pipeline
```

#### 3. Configurar las variables del entorno en un archivo `.env`:
```bash
# UID AIRFLOW
AIRFLOW_UID=50000 # Colocar siempre mismo valor

# APIs keys
api_key_coinmarketcap = 'complete_your_api_key'

# DB Redshift 
USER_REDSHIFT=your_username
PASSWORD_REDSHIFT=your_password
HOST_REDSHIFT=your_host
PORT_REDSHIFT=your_port
DBNAME_REDSHIFT=your_dbname

# Email-Alerting
ALERT_EMAIL = 'complete_your_email' # El proceso envia un mail de alerta para avisar status del pipeline, unicamente admite gmail
```

#### 4. Correr Makefile 

Este proceso efect√∫a una automatizaci√≥n del pipeline en lo que se refiere a la construcci√≥n de las imagenes de Docker, levantar los servicios de Docker compose y la creaci√≥n de directorios junto con la configuraci√≥n del entorno de Airflow.

```bash
make all
```
#### 5. Acceder a la UI de Airflow

Visita http://localhost:8080 en tu navegador. En el mismo vamos a tener que completar lo siguiente:

`usuario`: airflow

`Contrase√±a`: airflow

Podemos visualizarlo con m√°s atenci√≥n en este imagen:

![](https://github.com/TOMAS-IGNACIO-LATORRE/main-crypto-daily-statistics-pipeline/blob/main/airflow_2.png)

A continuaci√≥n, debemos configurar las variables de entorno para nuestro mail de gmail, para eso:

Admin > + Connections > + Add new record

En el mismo, se deben completar los siguientes registros:

 - `Connection Id`: smtp_default
 - `Connection Type`: Email
 - `Host`: smtp.gmail.com
 - `Login`: 'complete_your_email'
 - `Contrase√±a`: Para obtener la contrase√±a, deben generar una contrase√±a para aplicaciones en gmail. Para eso, deben acceder a https://myaccount.google.com/apppasswords y generar una contrase√±a
 - `Port`: 587
 - `Extra`: 
{
  "timeout": 30,
  "retry_limit": 5,
  "disable_tls": false,
  "disable_ssl": true
}

Para finalizar, bebemos prender encender el DAG que diga `crypto_price_dags` y esperar a las 00:00:00 del d√≠a siguiente para que corra o ejecutar manualmente el mismo.

> Si quieren aplicarlo para mails de outlook pueden visitar el siguiente video https://www.youtube.com/watch?v=D18G7hW8418 que me ayudo a aprender esta configuraci√≥n

## üåê Introducci√≥n

Este proyecto se encarga de la implementaci√≥n de un pipeline ETL (Extract, Transform, Load)  dise√±ado para obtener las cotizaciones de las principales criptomonedas por medio de extracciones diarias del d√≠a anterior. El pipeline utiliza ![Docker](https://img.shields.io/badge/Docker-blue?logo=docker&logoColor=white) para la contenerizaci√≥n, ![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-green?logo=apache-airflow&logoColor=white) para la orquestaci√≥n, ![Amazon Redshift](https://img.shields.io/badge/Amazon%20Redshift-red?logo=amazon-redshift&logoColor=white) como almacenamiento de datos y c√≥mputo. A su vez, se utiliza ![Python](https://img.shields.io/badge/Python-blue?logo=python&logoColor=white) como lenguaje principal a la hora de gestionar el pipeline extrayendo datos de APIs financieras(CoinGecko y CoinMarketCap), transformando en una etapa posterior y cargando en bases de datos para su uso anal√≠tico.

El objetivo principal de este proyecto es proporcionar un an√°lisis diario de las cotizaciones de las criptomonedas m√°s relevantes, obteniendo los datos del d√≠a anterior finalizado. A medida que las criptomonedas ganan popularidad y su uso se expande, es esencial monitorear sus precios y tendencias. Este an√°lisis ayuda a los inversores y a otros agentes econ√≥micos a tomar decisiones informadas en un entorno altamente vol√°til.

## üìà Pipeline de datos

En este proceso de ETL, se utiliz√≥ la visi√≥n de Databricks conocido como [Lakehouse](https://www.databricks.com/glossary/data-lakehouse). En esta visi√≥n, se implementa de la siguiente forma:

- **Staging**: Se realiza un proceso de extracci√≥n de tipo **batch processing** donde se obtienen los datos en bruto extra√≠dos directamente de las APIs de CoinGecko y CoinMarketCap. Estos datos se almacenan en archivos de tipo Parquet para su f√°cil manipulaci√≥n y lectura.
- **Silver**: En esta etapa, los datos se le realizan un proceso de enriquecimiento de datos, donde se combinan, limpian y transforman. Un ejemplo de estas transformaciones son agregaciones de columnas, modificaciones de tipos de datos, entre otras. Se obtienen dichos parquet transformados y se aplica una carga intermedia en Amazon RedShift para crear un conjunto de datos intermedios que facilitan el an√°lisis.
- **Gold**: Aqu√≠ se generan los conjuntos de datos finales que est√°n listos para el an√°lisis y la visualizaci√≥n. En este caso, se emplea Amazon RedShift para su an√°lisis.

 A continuaci√≥n, comparto una imagen que muestra esta estructura efectuada por DataBricks:

![](https://blog.bismart.com/hs-fs/hubfs/Arquitectura_Medallion_Pasos.jpg?width=1754&height=656&name=Arquitectura_Medallion_Pasos.jpg)

### üìÅ Fuente de datos hacia Staging
Para todas las fuentes, se utiliz√≥ c√≥digo en Python para obtener datos de las APIs. Estas son las siguientes:
  -  [CoinMarkerCap API](https://coinmarketcap.com/api/documentation/v1/): Desde esta fuente de datos obtenemos los datos descriptivos de cada criptomoneda. Entre ellos, podemos mencionar symbol, name, category (coin o token), description, logo, website y reddit. Esta API se accede mediante una API KEY, es necesario dirigirse a este [link](https://coinmarketcap.com/api/), es necesario registrarse y de esta manera, podemos tener acceso a su API gratuita. En el caso de solicitudes, tiene un l√≠mite de 333 request por d√≠a que significa un promedio de 10.000 llamadas por mes, esto se puede escalar si accedes a una versi√≥n pro de la API.

- [CoinGecko API](https://docs.coingecko.com/reference/introduction): Se trata de una API de acceso p√∫blico y gratuito sin necesidad de usar una API KEY. Desde esta fuente, extraemos los precios de apertura, de cierre, precios m√°ximos y m√≠nimos de cada cryptomoneda con frecuencia de media hora. La misma  impone l√≠mites en la frecuencia de las solicitudes que se trata de **100 solicitudes por minuto**.

> Es importante destacar que esta API de CoinGecko proporciona la informaci√≥n del d√≠a anterior para cada tipo de cambio, no datos hist√≥ricos de precios. Si se desea obtener datos de d√≠as anteriores, se debe modificar el par√°metro days ubicado en la carpeta staging en el archivo `api_extract_data.py`.

A nivel t√©cnico, en el DAG se cuenta con funci√≥n `run_staging` que se encarga de ejecutar esta extracci√≥n. Esta a su vez, llamada a dos funciones:

- `api_extract_data.py`: El archivo contiene dos funciones principales para interactuar con las APIs de CoinGecko y CoinMarketCap. La primera funci√≥n, get_crypto_ohlc_data, obtiene los precios OHLC de una criptomoneda para una fecha espec√≠fica, organizando los datos en un DataFrame de pandas. La segunda funci√≥n, create_crypto_table, extrae informaci√≥n descriptiva sobre una criptomoneda, manejando errores de solicitudes excesivas y asegurando una recuperaci√≥n adecuada de datos.

- `parquet_staging.py`: El archivo define una funci√≥n llamada parquet_create_staging, que se encarga de crear archivos Parquet para los precios diarios de criptomonedas y sus perfiles. Utiliza las funciones get_crypto_ohlc_data y create_crypto_table para obtener datos de criptomonedas, concatenando la informaci√≥n en DataFrames de pandas y manejando excepciones si no se recuperan datos v√°lidos. Al final, guarda los DataFrames en archivos Parquet en una ubicaci√≥n espec√≠fica, asegurando que se creen exitosamente.


Podemos visualizar este proceso en el siguiente esquema:

![](https://github.com/TOMAS-IGNACIO-LATORRE/main-crypto-daily-statistics-pipeline/blob/main/Source_to_Staging.png)

### üìÅ Staging hacia Silver

### üìÅ Silver hacia Gold

### üö® Alertas - email

## ‚ú® Futuras Mejoras
- Optimizaci√≥n de la ingesti√≥n de datos hist√≥ricos con procesamiento distribuido.
- Buscar APIS para poder incorporar el volumen del mercado diario de cada una de las criptomonedas y aprovechar para sacar m√©tricas de esta variable num√©rica.
