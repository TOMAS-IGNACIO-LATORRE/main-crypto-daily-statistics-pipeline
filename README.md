# main-crypto-daily-statistics-pipeline

## ğŸ“š Tabla de contenidos

1. [IntroducciÃ³n](#IntroducciÃ³n)
2. [Pipeline de datos](#Pipeline-de-datos)
3. [ImplementaciÃ³n](#ImplementaciÃ³n)

## ğŸŒ IntroducciÃ³n

Este proyecto se encarga de la implementaciÃ³n de un pipeline ETL (Extract, Transform, Load)  diseÃ±ado para obtener las cotizaciones de las principales criptomonedas por medio de extracciones diarias del dÃ­a anterior. El pipeline utiliza ![Docker](https://img.shields.io/badge/Docker-blue?logo=docker&logoColor=white) para la contenerizaciÃ³n, ![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-green?logo=apache-airflow&logoColor=white) para la orquestaciÃ³n, ![Amazon Redshift](https://img.shields.io/badge/Amazon%20Redshift-red?logo=amazon-redshift&logoColor=white) como almacenamiento de datos y cÃ³mputo. A su vez, se utiliza ![Python](https://img.shields.io/badge/Python-blue?logo=python&logoColor=white) como lenguaje principal a la hora de gestionar el pipeline extrayendo datos de APIs financieras(CoinGecko y CoinMarketCap), transformando en una etapa posterior y cargando en bases de datos para su uso analÃ­tico.

El objetivo principal de este proyecto es proporcionar un anÃ¡lisis diario de las cotizaciones de las criptomonedas mÃ¡s relevantes, obteniendo los datos del dÃ­a anterior finalizado. A medida que las criptomonedas ganan popularidad y su uso se expande, es esencial monitorear sus precios y tendencias. Este anÃ¡lisis ayuda a los inversores y a otros agentes econÃ³micos a tomar decisiones informadas en un entorno altamente volÃ¡til.

## ğŸ“ˆ Pipeline de datos

En este proceso de ETL, se utilizÃ³ la visiÃ³n de Databricks conocido como [Lakehouse](https://www.databricks.com/glossary/data-lakehouse). En esta visiÃ³n, se implementa de la siguiente forma:

- **Staging**: Se realiza un proceso de extracciÃ³n de tipo **batch processing** donde se obtienen los datos en bruto extraÃ­dos directamente de las APIs de CoinGecko y CoinMarketCap. Estos datos se almacenan en archivos de tipo Parquet para su fÃ¡cil manipulaciÃ³n y lectura.
- **Silver**: En esta etapa, los datos se le realizan un proceso de enriquecimiento de datos, donde se combinan, limpian y transforman. Un ejemplo de estas transformaciones son agregaciones de columnas, modificaciones de tipos de datos, entre otras. Se obtienen dichos parquet transformados y se aplica una carga intermedia en Amazon RedShift para crear un conjunto de datos intermedios que facilitan el anÃ¡lisis.
- **Gold**: AquÃ­ se generan los conjuntos de datos finales que estÃ¡n listos para el anÃ¡lisis y la visualizaciÃ³n. En este caso, se emplea Amazon RedShift para su anÃ¡lisis.

 A continuaciÃ³n, comparto una imagen que muestra esta estructura efectuada por DataBricks:

![](https://blog.bismart.com/hs-fs/hubfs/Arquitectura_Medallion_Pasos.jpg?width=1754&height=656&name=Arquitectura_Medallion_Pasos.jpg)

### ğŸ“ Fuente de datos hacia Staging
Para todas las fuentes, se utilizÃ³ cÃ³digo en Python para obtener datos de las APIs. Estas son las siguientes:
  -  [CoinMarkerCap API](https://coinmarketcap.com/api/documentation/v1/): Desde esta fuente de datos obtenemos los datos descriptivos de cada criptomoneda. Entre ellos, podemos mencionar symbol, name, category (coin o token), description, logo, website y reddit. Esta API se accede mediante una API KEY, es necesario dirigirse a este [link](https://coinmarketcap.com/api/), es necesario registrarse y de esta manera, podemos tener acceso a su API gratuita. En el caso de solicitudes, tiene un lÃ­mite de 333 request por dÃ­a que significa un promedio de 10.000 llamadas por mes, esto se puede escalar si accedes a una versiÃ³n pro de la API.

- [CoinGecko API](https://docs.coingecko.com/reference/introduction): Se trata de una API de acceso pÃºblico y gratuito sin necesidad de usar una API KEY. Desde esta fuente, extraemos los precios de apertura, de cierre, precios mÃ¡ximos y mÃ­nimos de cada cryptomoneda con frecuencia de media hora. La misma  impone lÃ­mites en la frecuencia de las solicitudes que se trata de **100 solicitudes por minuto**.

> Es importante destacar que esta API de CoinGecko proporciona la informaciÃ³n del dÃ­a anterior para cada tipo de cambio, no datos histÃ³ricos de precios. Si se desea obtener datos de dÃ­as anteriores, se debe modificar el parÃ¡metro days ubicado en la carpeta staging en el archivo `api_extract_data.py`.

A nivel tÃ©cnico, en el DAG se cuenta con funciÃ³n `run_staging` que se encarga de ejecutar esta extracciÃ³n. Esta a su vez, llamada a dos funciones:

- 


Podemos visualizar este proceso en el siguiente esquema:

![](https://github.com/TOMAS-IGNACIO-LATORRE/main-crypto-daily-statistics-pipeline/blob/main/Source_to_Staging.png)

### ğŸ“ Staging hacia Silver

### ğŸ“ Silver hacia Gold

### ğŸš¨ Alertas - email

## ğŸ› ï¸ ImplementaciÃ³n

###  Requisitos previos

Se debe tener instalado las siguientes herramientas:

- Python
- Docker Desktop
- Airflow
- AWS Redshift

## Setup