# main-crypto-daily-statistics-pipeline


## üìö Tabla de contenidos

1. [Introducci√≥n](#Introducci√≥n)
2. [Pipeline de datos](#Pipeline-de-datos)
3. [Implementaci√≥n](#Implementaci√≥n)

## üåê Introducci√≥n

Este proyecto se encarga de la implementaci√≥n de un pipeline ETL (Extract, Transform, Load) dise√±ado para obtener las cotizaciones de las principales criptomonedas por medio de extracciones diarias del d√≠a anterior. El pipeline utiliza ![Docker](https://img.shields.io/badge/Docker-blue?logo=docker&logoColor=white) para la contenerizaci√≥n, ![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-green?logo=apache-airflow&logoColor=white) para la orquestaci√≥n, ![Amazon Redshift](https://img.shields.io/badge/Amazon%20Redshift-red?logo=amazon-redshift&logoColor=white) como almacenamiento de datos y c√≥mputo. A su vez, se utiliza ![Python](https://img.shields.io/badge/Python-blue?logo=python&logoColor=white) como lenguaje principal a la hora de gestionar el pipeline extrayendo datos de APIs financieras(CoinGecko y CoinMarketCap), transformando en una etapa posterior y cargando en bases de datos para su uso anal√≠tico.

El objetivo principal de este proyecto es proporcionar un an√°lisis diario de las cotizaciones de las criptomonedas m√°s relevantes, obteniendo los datos del d√≠a anterior finalizado. A medida que las criptomonedas ganan popularidad y su uso se expande, es esencial monitorear sus precios y tendencias. Este an√°lisis ayuda a los inversores y a otros agentes econ√≥micos a tomar decisiones informadas en un entorno altamente vol√°til.

## üìà Pipeline de datos

En este proceso de ETL, se utiliz√≥ la visi√≥n de Databricks conocido como [Lakehouse](https://www.databricks.com/glossary/data-lakehouse). En esta visi√≥n, se inicia con las diversas **fuentes de datos** las cuales se extraen hacia una primera etapa conocida como **Bronze o Staging** en la cual se guardan los datos tal cual provienen de esta fuente de datos primaria. Luego, esa fuente primaria se le lleva hacia una segunda etapa conocida como **Silver** en la cual se aplican ciertas transformaciones como agregado de columnas, modificaci√≥n de tipos de datos, entre otras m√°s. Para finalizar, estos datos transformados se llevan hacia una etapa final conocida como **Gold** en la cual se disponibilizan los datos para emplear anal√≠tica sea mediante tableros de anal√≠tcas o modelos de Machine Learning. A continuaci√≥n, comparto una imagen que muestra esta estructura:

![](https://blog.bismart.com/hs-fs/hubfs/Arquitectura_Medallion_Pasos.jpg?width=1754&height=656&name=Arquitectura_Medallion_Pasos.jpg)

### üìÅ Fuente de datos hacia Staging
Para todas las fuentes, se utiliz√≥ c√≥digo en Python para obtener datos de las APIs. Entre las APIs utiza

### üìÅ Staging hacia Silver

### üìÅ Silver hacia Gold

### Alertas - email

## üõ†Ô∏è Implementaci√≥n

###  Requisitos previos

Se debe tener instalado las siguientes herramientas:

- Python
- Docker Desktop
- Airflow
- AWS Redshift

## Setup





